#pragma kernel LocalPrefixSum
#pragma kernel GlobalPrefixSum
#pragma kernel RadixReorder


#define GROUP_SIZE 512

StructuredBuffer<uint4> KeysIn;
RWStructuredBuffer<uint4> BucketsOut;
RWStructuredBuffer<uint> GlobalPrefixSumOut;
RWStructuredBuffer<uint> ValueScans;

StructuredBuffer<uint> GlobalPrefixSumIn;
StructuredBuffer<uint> ValueScansIn;
RWStructuredBuffer<uint> KeysOut;

//this buffer is 16 times the size the KeysIn buffer. It has the consecutive scans of the counts of 0's, 1's,..., 15's in the input array, in that order.

//Since one thread in LocalPrefixSum handles 4 input values, we need to output 4*16 (64) values to the ValueScan buffer per thread.

groupshared uint4x4 sharedBuffer[GROUP_SIZE];

int bitshift;

[numthreads(GROUP_SIZE, 1, 1)]
void LocalPrefixSum(uint3 DTid : SV_DispatchThreadID, uint3 groupId : SV_GroupID, uint GI : SV_GroupIndex)
{
    //take first four bits of every input element.
    uint4 keyBits = (KeysIn[DTid.x] >> bitshift) & 0xF;	//take the 4 least significant bits.  //hey this is only 16 bits, but it takes up 128 bits! ah we'll use it for counting so we need it to be potentially large.
    
	//if we're doing 4 bits (16 values) we need compare the 16 values to all values between 0-15 and return 0 or 1 depending on whether they are equal.

	uint4x4 miniBlock[4];
	miniBlock[0] = uint4x4(
		keyBits.xxxx == uint4(0u, 1u, 2u, 3u), keyBits.xxxx == uint4(4u, 5u, 6u, 7u), 
		keyBits.xxxx == uint4(8u, 9u, 10u, 11u), keyBits.xxxx == uint4(12u, 13u, 14u, 15u));

	miniBlock[1] = uint4x4(
		keyBits.yyyy == uint4(0u, 1u, 2u, 3u), keyBits.yyyy == uint4(4u, 5u, 6u, 7u), 
		keyBits.yyyy == uint4(8u, 9u, 10u, 11u), keyBits.yyyy == uint4(12u, 13u, 14u, 15u));

	miniBlock[2] = uint4x4(
		keyBits.zzzz == uint4(0u, 1u, 2u, 3u), keyBits.zzzz == uint4(4u, 5u, 6u, 7u), 
		keyBits.zzzz == uint4(8u, 9u, 10u, 11u), keyBits.zzzz == uint4(12u, 13u, 14u, 15u));

	miniBlock[3] = uint4x4(
		keyBits.wwww == uint4(0u, 1u, 2u, 3u), keyBits.wwww == uint4(4u, 5u, 6u, 7u), 
		keyBits.wwww == uint4(8u, 9u, 10u, 11u), keyBits.wwww == uint4(12u, 13u, 14u, 15u));

	//thread local scan: (sequential)
	[unroll(3)]
	for (int i = 0; i < 3; i++) { 
		miniBlock[i+1] += miniBlock[i];
	}

	sharedBuffer[GI] = miniBlock[3]; //counts of 0u, 1u, 2u, 3u, counts of 4u, 5u, 6u, 7u, counts of 8u, 9u, 10u, 11u, counts of 12u, 13u, 14u, 15u

	GroupMemoryBarrierWithGroupSync();

	//block local scan:
	//if we know how many threads there are in a group, we know how many times we need to do this step (this is probably not the fastest way to do intra block scan). 

	{
		[unroll(int(log2(GROUP_SIZE)))]
		for (uint i = 0u; i < uint(log2(GROUP_SIZE)); i++) {		//log2(256) == 8
			uint4x4 temp = sharedBuffer[GI];

			uint sumOffset = 1 << i;
			if (GI >= sumOffset) { //step0: ignore first elem, step1: ignore first 2 elems, step1: ignore first 4 elems, etc.
				temp += sharedBuffer[GI - sumOffset];
			}
			GroupMemoryBarrierWithGroupSync();
			sharedBuffer[GI] = temp;
			GroupMemoryBarrierWithGroupSync();
		}
	}

	//After this we have all the final sums for each 4 value subblock. Now we need to offset the partial sums with the previous subblocks final sum:

	if (GI > 0) {
		uint4x4 prevSubBlockFinalSum = sharedBuffer[GI-1];

		miniBlock[0] += prevSubBlockFinalSum;
		miniBlock[1] += prevSubBlockFinalSum;
		miniBlock[2] += prevSubBlockFinalSum;
		//miniBlock[3] += prevSubBlockFinalSum; // already stored in sharedBuffer[GI], right?
	}


	//it's not as simple as just getting some values. We need to make sure all the local prefix sums are adjusted to the global offsets.
	//Also in each of these threads we're dealing with 4 values that each should create 16 ValueScans values.

	//After the last GroupMemoryBarrierWithGroupSync we have all the local final sums though, so we can do some things by taking the last "groups" last element (biggest sum) as a base value for this "group".
	//These are not final global prefix sums, but that's fine, we can make the global in a later pass. They have "block scope".

	//sharedBuffer[GI][0].x is the block count for the 0's
	//sharedBuffer[GI][3].w is the block count for the 15's

	{
		uint offset = 64u * DTid.x;
		[unroll(4)]
		for (uint j = 0u; j < 4u; j++) {
			[unroll(4)]
			for (uint i = 0u; i < 4u; i++) {
				uint index = i + j * 4u;
				ValueScans[offset +       index] = miniBlock[0][j][i];
				ValueScans[offset + 16u + index] = miniBlock[1][j][i];
				ValueScans[offset + 32u + index] = miniBlock[2][j][i];
				ValueScans[offset + 48u + index] = sharedBuffer[GI][j][i];
			}
		}
	}

	//Maybe I should store all the values and not just the final one?
	if (GI == (GROUP_SIZE - 1u)) {
		BucketsOut[4u * groupId.x + 0u] = sharedBuffer[GI][0];	//counts of 0u, 1u, 2u, 3u
		BucketsOut[4u * groupId.x + 1u] = sharedBuffer[GI][1];	//counts of 4u, 5u, 6u, 7u
		BucketsOut[4u * groupId.x + 2u] = sharedBuffer[GI][2];	//counts of 8u, 9u, 10u, 11u
		BucketsOut[4u * groupId.x + 3u] = sharedBuffer[GI][3];	//counts of 12u, 13u, 14u, 15u
	}
}

//the number of thread groups will for most of our cases be at least as large as the thread group size. So we need to parallelize the global prefix sum as well.

//4 x uvec4 x NUMBER_OF_THREADGROUPS values. IF the number of values is higher than 4 x uvec4 x GROUP_SIZE, we have to do this global prefix sum in multiple steps, since all the values won't fit in a block.

[numthreads(GROUP_SIZE, 1, 1)]
void GlobalPrefixSum(uint3 DTid : SV_DispatchThreadID, uint3 groupId : SV_GroupID, uint GI : SV_GroupIndex)
{
	uint offset = 64u * DTid.x;
	uint4x4 miniBlock[4];
	[unroll(4)]
	for (uint j = 0u; j < 4u; j++) {
		[unroll(4)]
		for (uint i = 0u; i < 4u; i++) {
			uint index = i + j * 4u;
			miniBlock[0][j][i] = ValueScans[offset +       index];
			miniBlock[1][j][i] = ValueScans[offset + 16u + index];
			miniBlock[2][j][i] = ValueScans[offset + 32u + index];
			miniBlock[3][j][i] = ValueScans[offset + 48u + index];
		}
	}

	//Get the last element of each block (total block sums):
	//The following four uint4's contains the total sums of the values 0-15 (each value individually) from each block:
	//counts of 0u, 1u, 2u, 3u, counts of 4u, 5u, 6u, 7u, counts of 8u, 9u, 10u, 11u, counts of 12u, 13u, 14u, 15u
	//GI as an index only works (if it actually does?) here as long as the number of blocks is <= GROUP_SIZE.

	sharedBuffer[GI] = uint4x4(BucketsOut[4u * GI + 0u], BucketsOut[4u * GI + 1u], BucketsOut[4u * GI + 2u], BucketsOut[4u * GI + 3u]); 

	GroupMemoryBarrierWithGroupSync();

	//block local scan:
	//if we know how many threads there are in a group, we know how many times we need to do this step (this is probably not the fastest way to do intra block scan). 

	{
		[unroll(int(log2(GROUP_SIZE)))]
		for (uint i = 0u; i < uint(log2(GROUP_SIZE)); i++) {		//log2(256) == 8
			uint4x4 temp = sharedBuffer[GI];

			uint sumOffset = 1 << i;
			if (GI >= sumOffset) { //step0: ignore first elem, step1: ignore first 2 elems, step1: ignore first 4 elems, etc.
				temp += sharedBuffer[GI - sumOffset];
			}
			GroupMemoryBarrierWithGroupSync();
			sharedBuffer[GI] = temp;
			GroupMemoryBarrierWithGroupSync();
		}
	}

	//After this step the sharedBuffer contains partial and full prefix sums of all the blocks that could fit within one thread group (that is, GROUP_SIZE blocks).

	
	//here we need to take the value from the last BLOCK not the last thread. This complicates things because sharedBuffer is block scope only.
	//The shaderBuffer does contain the partial sums of the blocks though. This kernel was meant to be run only with one thread group, and have all the partial sums within one block.
		
	if (groupId.x > 0) {
		uint4x4 prevSubBlockFinalSum = sharedBuffer[groupId.x-1]; //this gives a new sum for each increase in GI. While we want the same for this group, right?

		[unroll(4)]
		for (uint i = 0u; i < 4u; i++) {
			miniBlock[i] += prevSubBlockFinalSum;
		}
	}

	{
		[unroll(4)]
		for (uint j = 0u; j < 4u; j++) {
			[unroll(4)]
			for (uint i = 0u; i < 4u; i++) {
				uint index = i + j * 4u;
				ValueScans[offset +       index] = miniBlock[0][j][i]; //miniBlock[0][0][0] contains the partial sum of 0's. It appears every 16's index of the ValueScans buffer.
				ValueScans[offset + 16u + index] = miniBlock[1][j][i];
				ValueScans[offset + 32u + index] = miniBlock[2][j][i];
				ValueScans[offset + 48u + index] = miniBlock[3][j][i];
			}
		}
	}

	if (groupId.x == 0 && GI == (GROUP_SIZE - 1u)) {
		//Here we scan the counts of the 16 values, in order to get their base indices.

		//exclusive scan:
		uint4 temp = sharedBuffer[GI][0];
		sharedBuffer[GI][0][0] = 0u;
		sharedBuffer[GI][0][1] = sharedBuffer[GI][0][0] + temp.x;
		sharedBuffer[GI][0][2] = sharedBuffer[GI][0][1] + temp.y;
		sharedBuffer[GI][0][3] = sharedBuffer[GI][0][2] + temp.z;

		uint4 temp2 = sharedBuffer[GI][1];
		sharedBuffer[GI][1][0] = sharedBuffer[GI][0][3] + temp.w;
		sharedBuffer[GI][1][1] = sharedBuffer[GI][1][0] + temp2.x;
		sharedBuffer[GI][1][2] = sharedBuffer[GI][1][1] + temp2.y;
		sharedBuffer[GI][1][3] = sharedBuffer[GI][1][2] + temp2.z;

		temp = sharedBuffer[GI][2];
		sharedBuffer[GI][2][0] = sharedBuffer[GI][1][3] + temp2.w;
		sharedBuffer[GI][2][1] = sharedBuffer[GI][2][0] + temp.x;
		sharedBuffer[GI][2][2] = sharedBuffer[GI][2][1] + temp.y;
		sharedBuffer[GI][2][3] = sharedBuffer[GI][2][2] + temp.z;

		temp2 = sharedBuffer[GI][3];
		sharedBuffer[GI][3][0] = sharedBuffer[GI][2][3] + temp.w;
		sharedBuffer[GI][3][1] = sharedBuffer[GI][3][0] + temp2.x;
		sharedBuffer[GI][3][2] = sharedBuffer[GI][3][1] + temp2.y;
		sharedBuffer[GI][3][3] = sharedBuffer[GI][3][2] + temp2.z;

		uint groupOffset = 16u * groupId.x;
		[unroll(4)]
		for (uint i = 0u; i < 4u; i++) {
			uint bufferOffset = i * 4u;
			GlobalPrefixSumOut[groupOffset + bufferOffset + 0u] = sharedBuffer[GI][i].x;	//counts of 0u, 1u, 2u, 3u
			GlobalPrefixSumOut[groupOffset + bufferOffset + 1u] = sharedBuffer[GI][i].y;	//counts of 0u, 1u, 2u, 3u
			GlobalPrefixSumOut[groupOffset + bufferOffset + 2u] = sharedBuffer[GI][i].z;	//counts of 0u, 1u, 2u, 3u
			GlobalPrefixSumOut[groupOffset + bufferOffset + 3u] = sharedBuffer[GI][i].w;	//counts of 0u, 1u, 2u, 3u
		}
	}
}


//GlobalPrefixSum returns the indiviual counts for each value 0-15, while we actually need the prefix sum of them.
//Is this actually correct? if it is, then I just need to run a prefix sum on the result, otherwise i need to change something.
//It is correct. It's just that the previous is "horizontal" and we need to do one last "vertical scan".
//For a 16K input array that looks like this: [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15, 0,1,2,3, ...], there will 1024 of each of the 16 elements.
//As a last step we need to do an exclusive scan over the 16 buckets, so we get this: 
//[0, 1024, 2048, 3072, 4096, 5120, ...], which we can use as direct index locations for the keyOut array.
//Since there are only 16 of these, we might as well do it in one thread.

//[2,3,5,4] -> [2,5,8,9], [0,2,5,8]

[numthreads(GROUP_SIZE, 1, 1)]
void RadixReorder(uint3 groupThreadId : SV_GroupThreadID, uint3 DTid : SV_DispatchThreadID, uint3 groupId : SV_GroupID, uint GI : SV_GroupIndex)
{
	uint4 keys = KeysIn[DTid.x];
	uint4 buckets = (keys >> bitshift) & 0xF;	

	[unroll(4)]
	for (uint i = 0u; i < 4u; i++) {
		uint globalOffset = GlobalPrefixSumIn[buckets[i]];
		uint offset = i * 16u;
		uint localOffset = ValueScansIn[64u * DTid.x + offset + buckets[i]] - 1u;  //ValueScansIn, every 16th value is a partial prefix sum of the same value.
		uint newIndex = globalOffset + localOffset;
		KeysOut[newIndex] = keys[i];
	}
}








